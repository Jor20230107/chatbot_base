{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "85772e44-8761-459b-8019-4bf63a2bec58",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline, Conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "99cc7457-f0cb-497c-b188-92b609f11be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "chatbot = pipeline(model=\"facebook/blenderbot-400M-distill\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "35f310e4-ea17-4de6-949f-7767a2258653",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation = Conversation(\"I'm looking for a movie - what's your favourite one?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "16cdaf94-349a-4589-8ca7-ef625a63f5ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Conversation id: a96e2cc2-c2e2-41fa-a15f-3f961d3781a5\n",
       "user: I'm looking for a movie - what's your favourite one?"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d43749ed-3a2b-4e09-8829-323cf7ad8835",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on Text2TextGenerationPipeline in module transformers.pipelines.text2text_generation object:\n",
      "\n",
      "class Text2TextGenerationPipeline(transformers.pipelines.base.Pipeline)\n",
      " |  Text2TextGenerationPipeline(*args, **kwargs)\n",
      " |  \n",
      " |  Pipeline for text to text generation using seq2seq models.\n",
      " |  \n",
      " |  Example:\n",
      " |  \n",
      " |  ```python\n",
      " |  >>> from transformers import pipeline\n",
      " |  \n",
      " |  >>> generator = pipeline(model=\"mrm8488/t5-base-finetuned-question-generation-ap\")\n",
      " |  >>> generator(\n",
      " |  ...     \"answer: Manuel context: Manuel has created RuPERTa-base with the support of HF-Transformers and Google\"\n",
      " |  ... )\n",
      " |  [{'generated_text': 'question: Who created the RuPERTa-base?'}]\n",
      " |  ```\n",
      " |  \n",
      " |  Learn more about the basics of using a pipeline in the [pipeline tutorial](../pipeline_tutorial). You can pass text\n",
      " |  generation parameters to this pipeline to control stopping criteria, decoding strategy, and more. Learn more about\n",
      " |  text generation parameters in [Text generation strategies](../generation_strategies) and [Text\n",
      " |  generation](text_generation).\n",
      " |  \n",
      " |  This Text2TextGenerationPipeline pipeline can currently be loaded from [`pipeline`] using the following task\n",
      " |  identifier: `\"text2text-generation\"`.\n",
      " |  \n",
      " |  The models that this pipeline can use are models that have been fine-tuned on a translation task. See the\n",
      " |  up-to-date list of available models on\n",
      " |  [huggingface.co/models](https://huggingface.co/models?filter=text2text-generation). For a list of available\n",
      " |  parameters, see the [following\n",
      " |  documentation](https://huggingface.co/docs/transformers/en/main_classes/text_generation#transformers.generation.GenerationMixin.generate)\n",
      " |  \n",
      " |  Usage:\n",
      " |  \n",
      " |  ```python\n",
      " |  text2text_generator = pipeline(\"text2text-generation\")\n",
      " |  text2text_generator(\"question: What is 42 ? context: 42 is the answer to life, the universe and everything\")\n",
      " |  ```\n",
      " |  Arguments:\n",
      " |      model ([`PreTrainedModel`] or [`TFPreTrainedModel`]):\n",
      " |          The model that will be used by the pipeline to make predictions. This needs to be a model inheriting from\n",
      " |          [`PreTrainedModel`] for PyTorch and [`TFPreTrainedModel`] for TensorFlow.\n",
      " |      tokenizer ([`PreTrainedTokenizer`]):\n",
      " |          The tokenizer that will be used by the pipeline to encode data for the model. This object inherits from\n",
      " |          [`PreTrainedTokenizer`].\n",
      " |      modelcard (`str` or [`ModelCard`], *optional*):\n",
      " |          Model card attributed to the model for this pipeline.\n",
      " |      framework (`str`, *optional*):\n",
      " |          The framework to use, either `\"pt\"` for PyTorch or `\"tf\"` for TensorFlow. The specified framework must be\n",
      " |          installed.\n",
      " |  \n",
      " |          If no framework is specified, will default to the one currently installed. If no framework is specified and\n",
      " |          both frameworks are installed, will default to the framework of the `model`, or to PyTorch if no model is\n",
      " |          provided.\n",
      " |      task (`str`, defaults to `\"\"`):\n",
      " |          A task-identifier for the pipeline.\n",
      " |      num_workers (`int`, *optional*, defaults to 8):\n",
      " |          When the pipeline will use *DataLoader* (when passing a dataset, on GPU for a Pytorch model), the number of\n",
      " |          workers to be used.\n",
      " |      batch_size (`int`, *optional*, defaults to 1):\n",
      " |          When the pipeline will use *DataLoader* (when passing a dataset, on GPU for a Pytorch model), the size of\n",
      " |          the batch to use, for inference this is not always beneficial, please read [Batching with\n",
      " |          pipelines](https://huggingface.co/transformers/main_classes/pipelines.html#pipeline-batching) .\n",
      " |      args_parser ([`~pipelines.ArgumentHandler`], *optional*):\n",
      " |          Reference to the object in charge of parsing supplied pipeline parameters.\n",
      " |      device (`int`, *optional*, defaults to -1):\n",
      " |          Device ordinal for CPU/GPU supports. Setting this to -1 will leverage CPU, a positive will run the model on\n",
      " |          the associated CUDA device id. You can pass native `torch.device` or a `str` too.\n",
      " |      binary_output (`bool`, *optional*, defaults to `False`):\n",
      " |          Flag indicating if the output the pipeline should happen in a binary format (i.e., pickle) or as raw text.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      Text2TextGenerationPipeline\n",
      " |      transformers.pipelines.base.Pipeline\n",
      " |      transformers.pipelines.base._ScikitCompat\n",
      " |      abc.ABC\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __call__(self, *args, **kwargs)\n",
      " |      Generate the output text(s) using text(s) given as inputs.\n",
      " |      \n",
      " |      Args:\n",
      " |          args (`str` or `List[str]`):\n",
      " |              Input text for the encoder.\n",
      " |          return_tensors (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether or not to include the tensors of predictions (as token indices) in the outputs.\n",
      " |          return_text (`bool`, *optional*, defaults to `True`):\n",
      " |              Whether or not to include the decoded texts in the outputs.\n",
      " |          clean_up_tokenization_spaces (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether or not to clean up the potential extra spaces in the text output.\n",
      " |          truncation (`TruncationStrategy`, *optional*, defaults to `TruncationStrategy.DO_NOT_TRUNCATE`):\n",
      " |              The truncation strategy for the tokenization within the pipeline. `TruncationStrategy.DO_NOT_TRUNCATE`\n",
      " |              (default) will never truncate, but it is sometimes desirable to truncate the input to fit the model's\n",
      " |              max_length instead of throwing an error down the line.\n",
      " |          generate_kwargs:\n",
      " |              Additional keyword arguments to pass along to the generate method of the model (see the generate method\n",
      " |              corresponding to your framework [here](./model#generative-models)).\n",
      " |      \n",
      " |      Return:\n",
      " |          A list or a list of list of `dict`: Each result comes as a dictionary with the following keys:\n",
      " |      \n",
      " |          - **generated_text** (`str`, present when `return_text=True`) -- The generated text.\n",
      " |          - **generated_token_ids** (`torch.Tensor` or `tf.Tensor`, present when `return_tensors=True`) -- The token\n",
      " |            ids of the generated text.\n",
      " |  \n",
      " |  __init__(self, *args, **kwargs)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  check_inputs(self, input_length: int, min_length: int, max_length: int)\n",
      " |      Checks whether there might be something wrong with given input with regard to the model.\n",
      " |  \n",
      " |  postprocess(self, model_outputs, return_type=<ReturnType.TEXT: 1>, clean_up_tokenization_spaces=False)\n",
      " |      Postprocess will receive the raw outputs of the `_forward` method, generally tensors, and reformat them into\n",
      " |      something more friendly. Generally it will output a list or a dict or results (containing just strings and\n",
      " |      numbers).\n",
      " |  \n",
      " |  preprocess(self, inputs, truncation=<TruncationStrategy.DO_NOT_TRUNCATE: 'do_not_truncate'>, **kwargs)\n",
      " |      Preprocess will take the `input_` of a specific pipeline and return a dictionary of everything necessary for\n",
      " |      `_forward` to run properly. It should contain at least one tensor, but might have arbitrary other items.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __abstractmethods__ = frozenset()\n",
      " |  \n",
      " |  return_name = 'generated'\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from transformers.pipelines.base.Pipeline:\n",
      " |  \n",
      " |  check_model_type(self, supported_models: Union[List[str], dict])\n",
      " |      Check if the model class is in supported by the pipeline.\n",
      " |      \n",
      " |      Args:\n",
      " |          supported_models (`List[str]` or `dict`):\n",
      " |              The list of models supported by the pipeline, or a dictionary with model class values.\n",
      " |  \n",
      " |  device_placement(self)\n",
      " |      Context Manager allowing tensor allocation on the user-specified device in framework agnostic way.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Context manager\n",
      " |      \n",
      " |      Examples:\n",
      " |      \n",
      " |      ```python\n",
      " |      # Explicitly ask for tensor allocation on CUDA device :0\n",
      " |      pipe = pipeline(..., device=0)\n",
      " |      with pipe.device_placement():\n",
      " |          # Every framework specific tensor allocation will be done on the request device\n",
      " |          output = pipe(...)\n",
      " |      ```\n",
      " |  \n",
      " |  ensure_tensor_on_device(self, **inputs)\n",
      " |      Ensure PyTorch tensors are on the specified device.\n",
      " |      \n",
      " |      Args:\n",
      " |          inputs (keyword arguments that should be `torch.Tensor`, the rest is ignored):\n",
      " |              The tensors to place on `self.device`.\n",
      " |          Recursive on lists **only**.\n",
      " |      \n",
      " |      Return:\n",
      " |          `Dict[str, torch.Tensor]`: The same as `inputs` but on the proper device.\n",
      " |  \n",
      " |  forward(self, model_inputs, **forward_params)\n",
      " |  \n",
      " |  get_inference_context(self)\n",
      " |  \n",
      " |  get_iterator(self, inputs, num_workers: int, batch_size: int, preprocess_params, forward_params, postprocess_params)\n",
      " |  \n",
      " |  iterate(self, inputs, preprocess_params, forward_params, postprocess_params)\n",
      " |  \n",
      " |  predict(self, X)\n",
      " |      Scikit / Keras interface to transformers' pipelines. This method will forward to __call__().\n",
      " |  \n",
      " |  run_multi(self, inputs, preprocess_params, forward_params, postprocess_params)\n",
      " |  \n",
      " |  run_single(self, inputs, preprocess_params, forward_params, postprocess_params)\n",
      " |  \n",
      " |  save_pretrained(self, save_directory: str, safe_serialization: bool = True)\n",
      " |      Save the pipeline's model and tokenizer.\n",
      " |      \n",
      " |      Args:\n",
      " |          save_directory (`str`):\n",
      " |              A path to the directory where to saved. It will be created if it doesn't exist.\n",
      " |          safe_serialization (`str`):\n",
      " |              Whether to save the model using `safetensors` or the traditional way for PyTorch or Tensorflow.\n",
      " |  \n",
      " |  transform(self, X)\n",
      " |      Scikit / Keras interface to transformers' pipelines. This method will forward to __call__().\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from transformers.pipelines.base.Pipeline:\n",
      " |  \n",
      " |  default_input_names = None\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from transformers.pipelines.base._ScikitCompat:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(chatbot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0a24627c-95cf-4902-b991-ad4d9833797f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generated_text': \" I don't really have a favorite movie, but I do like action movies. What about you?\"}]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = chatbot(\"I'm looking for a movie - what's your favourite one?\")\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "723b762b-afcf-4e05-8ecc-5b841b3b1988",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_items([('generated_text', \" I don't really have a favorite movie, but I do like action movies. What about you?\")])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res[-1].items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "886ac16e-7233-4adc-9f03-3021621bf64b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_values([\" I don't really have a favorite movie, but I do like action movies. What about you?\"])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res[-1].values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9f6d5e44-a499-41a2-aaba-9b5382a1e73d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'generated_text': \" I don't really have a favorite movie, but I do like action movies. What about you?\"}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8c99c049-ea3d-4a1b-916a-3b5a3caa43b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" I don't really have a favorite movie, but I do like action movies. What about you?\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res[-1]['generated_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7549d1d4-f559-4266-a9b6-37765502ae46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Conversation id: a96e2cc2-c2e2-41fa-a15f-3f961d3781a5\\nuser: I'm looking for a movie - what's your favourite one?\\n\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(conversation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "799ccb01-1c2f-4c79-b588-93b1c97d1d26",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": " `args[0]`: Conversation id: a96e2cc2-c2e2-41fa-a15f-3f961d3781a5\nuser: I'm looking for a movie - what's your favourite one?\n have the wrong format. The should be either of type `str` or type `list`",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m conversation \u001b[38;5;241m=\u001b[39m \u001b[43mchatbot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconversation\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\users\\82104\\documents\\web\\github\\doit\\venv\\lib\\site-packages\\transformers\\pipelines\\text2text_generation.py:167\u001b[0m, in \u001b[0;36mText2TextGenerationPipeline.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    138\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    139\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    140\u001b[0m \u001b[38;5;124;03m    Generate the output text(s) using text(s) given as inputs.\u001b[39;00m\n\u001b[0;32m    141\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    164\u001b[0m \u001b[38;5;124;03m          ids of the generated text.\u001b[39;00m\n\u001b[0;32m    165\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 167\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    168\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    169\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(args[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mlist\u001b[39m)\n\u001b[0;32m    170\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(el, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m el \u001b[38;5;129;01min\u001b[39;00m args[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m    171\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\u001b[38;5;28mlen\u001b[39m(res) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m result)\n\u001b[0;32m    172\u001b[0m     ):\n\u001b[0;32m    173\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [res[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m result]\n",
      "File \u001b[1;32mc:\\users\\82104\\documents\\web\\github\\doit\\venv\\lib\\site-packages\\transformers\\pipelines\\base.py:1140\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[1;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1132\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mnext\u001b[39m(\n\u001b[0;32m   1133\u001b[0m         \u001b[38;5;28miter\u001b[39m(\n\u001b[0;32m   1134\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_iterator(\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1137\u001b[0m         )\n\u001b[0;32m   1138\u001b[0m     )\n\u001b[0;32m   1139\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1140\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_single\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreprocess_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforward_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpostprocess_params\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\users\\82104\\documents\\web\\github\\doit\\venv\\lib\\site-packages\\transformers\\pipelines\\base.py:1146\u001b[0m, in \u001b[0;36mPipeline.run_single\u001b[1;34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001b[0m\n\u001b[0;32m   1145\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun_single\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs, preprocess_params, forward_params, postprocess_params):\n\u001b[1;32m-> 1146\u001b[0m     model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpreprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpreprocess_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1147\u001b[0m     model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward(model_inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mforward_params)\n\u001b[0;32m   1148\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpostprocess(model_outputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpostprocess_params)\n",
      "File \u001b[1;32mc:\\users\\82104\\documents\\web\\github\\doit\\venv\\lib\\site-packages\\transformers\\pipelines\\text2text_generation.py:177\u001b[0m, in \u001b[0;36mText2TextGenerationPipeline.preprocess\u001b[1;34m(self, inputs, truncation, **kwargs)\u001b[0m\n\u001b[0;32m    176\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpreprocess\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs, truncation\u001b[38;5;241m=\u001b[39mTruncationStrategy\u001b[38;5;241m.\u001b[39mDO_NOT_TRUNCATE, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 177\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parse_and_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtruncation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtruncation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    178\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m inputs\n",
      "File \u001b[1;32mc:\\users\\82104\\documents\\web\\github\\doit\\venv\\lib\\site-packages\\transformers\\pipelines\\text2text_generation.py:129\u001b[0m, in \u001b[0;36mText2TextGenerationPipeline._parse_and_tokenize\u001b[1;34m(self, truncation, *args)\u001b[0m\n\u001b[0;32m    127\u001b[0m     padding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    128\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 129\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    130\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m `args[0]`: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00margs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m have the wrong format. The should be either of type `str` or type `list`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    131\u001b[0m     )\n\u001b[0;32m    132\u001b[0m inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer(\u001b[38;5;241m*\u001b[39margs, padding\u001b[38;5;241m=\u001b[39mpadding, truncation\u001b[38;5;241m=\u001b[39mtruncation, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframework)\n\u001b[0;32m    133\u001b[0m \u001b[38;5;66;03m# This is produced by tokenizers but is an invalid generate kwargs\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m:  `args[0]`: Conversation id: a96e2cc2-c2e2-41fa-a15f-3f961d3781a5\nuser: I'm looking for a movie - what's your favourite one?\n have the wrong format. The should be either of type `str` or type `list`"
     ]
    }
   ],
   "source": [
    "conversation = chatbot(conversation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d02a37d3-5d4b-405c-adf3-1e7ea2a1bb51",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
